# References (Seed)

Foundational Architectures

- [ResNet2016] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. CVPR. ([verification](../../papers/ResNet2016.md))
- [Transformer2017] Vaswani, A. et al. (2017). Attention Is All You Need. NeurIPS. ([verification](../../papers/Transformer2017.md))
- [U-Net2015] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI. ([verification](../../papers/UNet2015.md))

Normalization

- [BatchNorm2015] Ioffe, S., & Szegedy, C. (2015). Batch Normalization. ICML. ([verification](../../papers/BatchNorm2015.md))
- [LayerNorm2016] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. arXiv. ([verification](../../papers/LayerNorm2016.md))
- [RMSNorm2023] Touvron, H. et al. (2023). RMSNorm (usage in LLaMA family). arXiv. ([verification](../../papers/RMSNorm2023.md))

Adapters & Parameter-Efficient Tuning

- [Adapters2021] Pfeiffer, J. et al. (2021). AdapterFusion: Non-Destructive Task Composition. ACL. ([verification](../../papers/Adapters2021.md))
- [LoRA2022] Hu, E. J. et al. (2022). LoRA: Low-Rank Adaptation of Large Language Models. ICLR. ([verification](../../papers/LoRA2022.md))
- [PrefixTuning2021] Li, X. L., & Liang, P. (2021). Prefix-Tuning. ACL. ([verification](../../papers/PrefixTuning2021.md))
- [PromptTuning2021] Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. EMNLP. ([verification](../../papers/PromptTuning2021.md))

Attention & Efficiency

- [Performer2020] Choromanski, K. et al. (2020). Rethinking Attention with Performers. ICLR. ([verification](../../papers/Performer2020.md))
- [SparseMoE2021] Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch Transformers. JMLR. ([verification](../../papers/SparseMoE2021.md))

Memory & Hypernetworks

- [NTM2014] Graves, A. et al. (2014). Neural Turing Machines. arXiv. ([verification](../../papers/NTM2014.md))
- [Hypernetworks2017] Ha, D., Dai, A., & Le, Q. V. (2017). HyperNetworks. ICLR. ([verification](../../papers/HyperNetworks2017.md))

Domain Adaptation & Constraints

- [DANN2016] Ganin, Y. et al. (2016). Domain-Adversarial Training of Neural Networks. JMLR. ([verification](../../papers/DANN2016.md))
- [Quantization2018] Jacob, B. et al. (2018). Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. CVPR. ([verification](../../papers/Quantization2018.md))

Geometry & Representation

- [Poincare2017] Nickel, M., & Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. NeurIPS. ([verification](../../papers/Poincare2017.md))
- [SlotAttention2020] Locatello, F. et al. (2020). Object-Centric Learning with Slot Attention. NeurIPS. ([verification](../../papers/SlotAttention2020.md))

Note: Expand to ≥ 30 curated entries in Milestone 4. Use keys in square brackets for in-text citations. Each key now links to a verification file under `papers/` confirming existence and claim accuracy.
