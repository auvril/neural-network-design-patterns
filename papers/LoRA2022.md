# LoRA2022 Verification

Key: [LoRA2022]
Title: LoRA: Low-Rank Adaptation of Large Language Models
Authors: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
Venue: ICLR 2022
DOI: (ICLR proceedings) / arXiv:2106.09685
URL: <https://arxiv.org/abs/2106.09685>
PDF: <https://arxiv.org/pdf/2106.09685>

## Abstract (excerpt)

"Low-Rank Adaptation (LoRA) freezes pre-trained model weights and injects trainable rank-decomposition matrices into Transformer layers, drastically reducing the number of trainable parameters for downstream tasks while maintaining performance."

## Claim Verification Against `references.md`

Reference entry claim (LoRA 2022) correctly identifies a parameter-efficient fine-tuning technique using low-rank updates to weight matrices.

Verified Points:

- Injects low-rank matrices (A, B) such that effective weight update ΔW = B A.
- Base weights frozen; only small rank parameters trained → memory/compute efficiency.
- Demonstrated strong performance with large reduction in trainable parameters compared to full fine-tuning.
- Compatible with other PEFT methods (e.g., adapters, prompts) conceptually.

No correction needed.

## Notable Contributions

- Formal low-rank reparameterization for adaptation.
- Scalability: enables fine-tuning very large models on modest hardware.
- Empirical evaluation across NLP tasks with minimal quality loss.

## Follow-on Patterns

- Extensions: QLoRA (quantization + LoRA), PiSSA, Sparse/Selective LoRA variants, LoRA-FA.
- Incorporated widely in open-source LLM fine-tuning stacks (HuggingFace PEFT, etc.).

## BibTeX

```bibtex
@inproceedings{hu2022lora,
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://arxiv.org/abs/2106.09685}
}
```
